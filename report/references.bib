@misc{gruver_large_2024,
	title = {Large {Language} {Models} {Are} {Zero}-{Shot} {Time} {Series} {Forecasters}},
	url = {http://arxiv.org/abs/2310.07820},
	doi = {10.48550/arXiv.2310.07820},
	abstract = {By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zeroshot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew Gordon},
	month = aug,
	year = {2024},
	note = {arXiv:2310.07820 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2023. Code available at: https://github.com/ngruver/llmtime},
	file = {Gruver et al. - 2024 - Large Language Models Are Zero-Shot Time Series Fo.pdf:C\:\\Users\\Jack\\Zotero\\storage\\L5KJTITQ\\Gruver et al. - 2024 - Large Language Models Are Zero-Shot Time Series Fo.pdf:application/pdf},
}

@misc{qwen_qwen25_2025,
	title = {Qwen2.5 {Technical} {Report}},
	url = {http://arxiv.org/abs/2412.15115},
	doi = {10.48550/arXiv.2412.15115},
	abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning, including offline learning DPO and online learning GRPO. Post-training techniques significantly enhance human preference, and notably improve long text generation, structural data analysis, and instruction following.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
	month = jan,
	year = {2025},
	note = {arXiv:2412.15115 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Qwen et al. - 2025 - Qwen2.5 Technical Report.pdf:C\:\\Users\\Jack\\Zotero\\storage\\CEPW64S6\\Qwen et al. - 2025 - Qwen2.5 Technical Report.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2023 - Attention Is All You Need.pdf:C\:\\Users\\Jack\\Zotero\\storage\\FS3KML39\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{zhang_root_2019,
	title = {Root {Mean} {Square} {Layer} {Normalization}},
	url = {http://arxiv.org/abs/1910.07467},
	doi = {10.48550/arXiv.1910.07467},
	abstract = {Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and signiﬁcantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efﬁcient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p\% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7\%∼64\% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Zhang, Biao and Sennrich, Rico},
	month = oct,
	year = {2019},
	note = {arXiv:1910.07467 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019},
	file = {Zhang and Sennrich - 2019 - Root Mean Square Layer Normalization.pdf:C\:\\Users\\Jack\\Zotero\\storage\\ZM4VYE6C\\Zhang and Sennrich - 2019 - Root Mean Square Layer Normalization.pdf:application/pdf},
}

@misc{su_roformer_2023,
	title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
	shorttitle = {{RoFormer}},
	url = {http://arxiv.org/abs/2104.09864},
	doi = {10.48550/arXiv.2104.09864},
	abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model\_doc/roformer.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	month = nov,
	year = {2023},
	note = {arXiv:2104.09864 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: fixed some typos},
	file = {Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Positio.pdf:C\:\\Users\\Jack\\Zotero\\storage\\U6JJK6FY\\Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Positio.pdf:application/pdf},
}

@misc{elfwing_sigmoid-weighted_2017,
	title = {Sigmoid-{Weighted} {Linear} {Units} for {Neural} {Network} {Function} {Approximation} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1702.03118},
	doi = {10.48550/arXiv.1702.03118},
	abstract = {In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro's TD-Gammon achieved near toplevel human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, ﬁrst, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10×10 board, using TD(λ) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(λ) agent with SiLU and dSiLU hidden units.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	month = nov,
	year = {2017},
	note = {arXiv:1702.03118 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 18 pages, 22 figures; added deep RL results for SZ-Tetris},
	file = {Elfwing et al. - 2017 - Sigmoid-Weighted Linear Units for Neural Network F.pdf:C\:\\Users\\Jack\\Zotero\\storage\\2H8JV7FX\\Elfwing et al. - 2017 - Sigmoid-Weighted Linear Units for Neural Network F.pdf:application/pdf},
}

@misc{agarap_deep_2019,
	title = {Deep {Learning} using {Rectified} {Linear} {Units} ({ReLU})},
	url = {http://arxiv.org/abs/1803.08375},
	doi = {10.48550/arXiv.1803.08375},
	abstract = {We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer hn−1 in a neural network, then multiply it by weight parameters θ to get the raw scores oi . Afterwards, we threshold the raw scores oi by 0, i.e. f (o) = max(0, oi ), where f (o) is the ReLU function. We provide class predictions yˆ through arg max function, i.e. arg max f (x).},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Agarap, Abien Fred},
	month = feb,
	year = {2019},
	note = {arXiv:1803.08375 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 7 pages, 11 figures, 9 tables},
	file = {Agarap - 2019 - Deep Learning using Rectified Linear Units (ReLU).pdf:C\:\\Users\\Jack\\Zotero\\storage\\MMXLD5X7\\Agarap - 2019 - Deep Learning using Rectified Linear Units (ReLU).pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:C\:\\Users\\Jack\\Zotero\\storage\\S3Z8HF4J\\Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	doi = {10.48550/arXiv.1502.01852},
	abstract = {Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit (PReLU) that generalizes the traditional rectiﬁed unit. PReLU improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classiﬁcation dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [29]). To our knowledge, our result is the ﬁrst to surpass human-level performance (5.1\%, [22]) on this visual recognition challenge.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv:1502.01852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:C\:\\Users\\Jack\\Zotero\\storage\\HRXVNDGJ\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf},
}

@misc{herzen_darts_2022,
	title = {Darts: {User}-{Friendly} {Modern} {Machine} {Learning} for {Time} {Series}},
	shorttitle = {Darts},
	url = {http://arxiv.org/abs/2110.03224},
	doi = {10.48550/arXiv.2110.03224},
	abstract = {We present Darts1, a Python machine learning library for time series, with a focus on forecasting. Darts oﬀers a variety of models, from classics such as ARIMA to state-of-the-art deep neural networks. The emphasis of the library is on oﬀering modern machine learning functionalities, such as supporting multidimensional series, ﬁtting models on multiple series, training on large datasets, incorporating external data, ensembling models, and providing a rich support for probabilistic forecasting. At the same time, great care goes into the API design to make it user-friendly and easy to use. For instance, all models can be used using fit()/predict(), similar to scikit-learn (Pedregosa et al., 2011).},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Herzen, Julien and Lässig, Francesco and Piazzetta, Samuele Giuliano and Neuer, Thomas and Tafti, Léo and Raille, Guillaume and Pottelbergh, Tomas Van and Pasieka, Marek and Skrodzki, Andrzej and Huguenin, Nicolas and Dumonal, Maxime and Kościsz, Jan and Bader, Dennis and Gusset, Frédérick and Benheddi, Mounir and Williamson, Camila and Kosinski, Michal and Petrik, Matej and Grosch, Gaël},
	month = may,
	year = {2022},
	note = {arXiv:2110.03224 [cs]},
	keywords = {Statistics - Computation, Computer Science - Machine Learning},
	annote = {Comment: Darts Github repository: https://github.com/unit8co/darts},
	file = {Herzen et al. - 2022 - Darts User-Friendly Modern Machine Learning for T.pdf:C\:\\Users\\Jack\\Zotero\\storage\\7W2IAWBS\\Herzen et al. - 2022 - Darts User-Friendly Modern Machine Learning for T.pdf:application/pdf},
}

@misc{godahewa_monash_2021,
	title = {Monash {Time} {Series} {Forecasting} {Archive}},
	url = {http://arxiv.org/abs/2105.06643},
	doi = {10.48550/arXiv.2105.06643},
	abstract = {Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets. In this paper, we present such a comprehensive time series forecasting archive containing 20 publicly available time series datasets from varied domains, with diﬀerent characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and diﬀerences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across eight error metrics, for the beneﬁt of researchers using the archive to benchmark their forecasting algorithms.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Godahewa, Rakshitha and Bergmeir, Christoph and Webb, Geoffrey I. and Hyndman, Rob J. and Montero-Manso, Pablo},
	month = may,
	year = {2021},
	note = {arXiv:2105.06643 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 33 pages, 3 figures, 15 tables},
	file = {Godahewa et al. - 2021 - Monash Time Series Forecasting Archive.pdf:C\:\\Users\\Jack\\Zotero\\storage\\CJSV9PAI\\Godahewa et al. - 2021 - Monash Time Series Forecasting Archive.pdf:application/pdf},
}

@misc{zhou_informer_2021,
	title = {Informer: {Beyond} {Efficient} {Transformer} for {Long} {Sequence} {Time}-{Series} {Forecasting}},
	shorttitle = {Informer},
	url = {http://arxiv.org/abs/2012.07436},
	doi = {10.48550/arXiv.2012.07436},
	abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efﬁciently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efﬁcient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efﬁciently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer signiﬁcantly outperforms existing methods and provides a new solution to the LSTF problem.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
	month = mar,
	year = {2021},
	note = {arXiv:2012.07436 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	annote = {Comment: 8 pages (main), 5 pages (appendix) and to be appeared in AAAI2021},
	file = {Zhou et al. - 2021 - Informer Beyond Efficient Transformer for Long Se.pdf:C\:\\Users\\Jack\\Zotero\\storage\\MAWI98PL\\Zhou et al. - 2021 - Informer Beyond Efficient Transformer for Long Se.pdf:application/pdf},
}

@misc{toner_performance_2025,
	title = {Performance of {Zero}-{Shot} {Time} {Series} {Foundation} {Models} on {Cloud} {Data}},
	url = {http://arxiv.org/abs/2502.12944},
	doi = {10.48550/arXiv.2502.12944},
	abstract = {Time series foundation models (FMs) have emerged as a popular paradigm for zero-shot multi-domain forecasting. FMs are trained on numerous diverse datasets and claim to be effective forecasters across multiple different time series domains, including cloud data. In this work we investigate this claim, exploring the effectiveness of FMs on cloud data. We demonstrate that many well-known FMs fail to generate meaningful or accurate zero-shot forecasts in this setting. We support this claim empirically, showing that FMs are outperformed consistently by simple linear baselines. We also illustrate a number of interesting pathologies, including instances where FMs suddenly output seemingly erratic, random-looking forecasts. Our results suggest a widespread failure of FMs to model cloud data.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Toner, William and Lee, Thomas L. and Joosen, Artjom and Singh, Rajkarn and Asenov, Martin},
	month = mar,
	year = {2025},
	note = {arXiv:2502.12944 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 5 pages, Preprint},
	file = {Toner et al. - 2025 - Performance of Zero-Shot Time Series Foundation Mo.pdf:C\:\\Users\\Jack\\Zotero\\storage\\ACM4IAHA\\Toner et al. - 2025 - Performance of Zero-Shot Time Series Foundation Mo.pdf:application/pdf},
}

@misc{chen_unleashing_2024,
	title = {Unleashing the potential of prompt engineering in {Large} {Language} {Models}: a comprehensive review},
	shorttitle = {Unleashing the potential of prompt engineering in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.14735},
	doi = {10.48550/arXiv.2310.14735},
	abstract = {This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). The development of Artificial Intelligence (AI), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in VisionLanguage Models (VLMs), with models such as CLIP and ALIGN. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of VLMs through innovative approaches such as Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed, through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.},
	language = {en},
	urldate = {2025-04-01},
	publisher = {arXiv},
	author = {Chen, Banghao and Zhang, Zhaofeng and Langrené, Nicolas and Zhu, Shengxin},
	month = sep,
	year = {2024},
	note = {arXiv:2310.14735 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Chen et al. - 2024 - Unleashing the potential of prompt engineering in .pdf:C\:\\Users\\Jack\\Zotero\\storage\\NEB242JZ\\Chen et al. - 2024 - Unleashing the potential of prompt engineering in .pdf:application/pdf},
}
